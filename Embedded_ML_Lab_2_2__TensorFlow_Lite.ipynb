{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Embedded ML - Lab 2.2: TensorFlow Lite"
      ],
      "metadata": {
        "id": "7SFBFiQlYlva"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Jose Andres Henao Alzate#\n",
        "\n",
        "\n",
        "#CC 1036686332#"
      ],
      "metadata": {
        "id": "4X5y_xTPPscU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab you will learn the basics of TensorFlow Lite, a complement of TensorFlow that allows you to optimize and run models on constrained devices. It provides a much lighter runtime than TensorFlow but it only supports a subset of the tools available in full TensorFlow.\n",
        "\n",
        "In this lab you might be given some helper functions but you are expected to write most of the code and be able to explain it at a high level of abstraction and also to modify any part of it."
      ],
      "metadata": {
        "id": "svldvvGfmN8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning outcomes\n",
        "\n",
        "\n",
        "* Explain the basic concepts associated with TensorFlow Lite\n",
        "* Develop applications following the basic TensorFlow Lite workflow\n",
        "* Implement post-training quantization using TensorFlow Lite tools"
      ],
      "metadata": {
        "id": "lQK0RRRuY3rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TensorFlow Lite workflow\n",
        "After having built a TensorFlow model, you can convert it to the TensorFlow Lite representation. Then you can run it with the TensorFlow Lite interpreter on your development environment before exporting it and copying it to the target device.\n",
        "\n",
        "To run the model with TensorFlow Lite you should load the model to the TensorFlow Lite interpreter, allocate the input/output tensors, pass the input data and finally run inference. Notice that TensorFlow Lite API calls are different from those of TensorFlow.\n",
        "\n",
        "In this part of the assignment, you should create and train a simple model (e.g. a one-neuron network) with TensorFlow and then save it. Then follow the TensorFlow Lite workflow until you are able to run inference and validate the outputs."
      ],
      "metadata": {
        "id": "l8wat6Kxul5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import layers, models\n",
        "import pathlib\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
      ],
      "metadata": {
        "id": "ItciB60YrZp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### TENSORFLOW BASIC WORKFLOW\n",
        "\n",
        "# Datos de entrenamiento\n",
        "voltajes_train = [1.2, 2.4, 3.1, 4.0, 5.2, 6.0, 7.1, 8.3, 9.0, 10.1,\n",
        "                  1.0, 2.2, 3.0, 3.9, 5.0, 6.3, 7.0, 8.1, 9.3, 10.5]\n",
        "corrientes_train = [0.12, 0.24, 0.31, 0.40, 0.51, 0.60, 0.71, 0.84, 0.88, 1.00,\n",
        "                    0.10, 0.22, 0.29, 0.39, 0.50, 0.61, 0.70, 0.81, 0.93, 1.05]\n",
        "\n",
        "# Datos de prueba\n",
        "voltajes_test = [1.5, 2.6, 3.5, 4.5, 5.5, 6.5, 7.5, 8.5, 9.5, 10.8,\n",
        "                 1.1, 2.3, 3.3, 4.2, 5.3, 6.4, 7.4, 8.6, 9.7, 10.9]\n",
        "corrientes_test = [0.15, 0.26, 0.35, 0.45, 0.55, 0.65, 0.75, 0.85, 0.95, 1.08,\n",
        "                   0.11, 0.23, 0.33, 0.42, 0.53, 0.64, 0.74, 0.86, 0.97, 1.09]\n",
        "\n",
        "\n",
        "#Datos en arrays de numpy\n",
        "X_train = np.array(voltajes_train)\n",
        "y_train = np.array(corrientes_train)\n",
        "X_test = np.array(voltajes_test)\n",
        "y_test = np.array(corrientes_test)\n",
        "\n",
        "# Create the model\n",
        "model= keras.Sequential([\n",
        "    keras.layers.Dense(1,input_shape=[1]),\n",
        "    keras.layers.Dense(10,activation='linear'),\n",
        "    keras.layers.Dense(1)\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='mean_squared_error',\n",
        "              metrics=['mean_absolute_error'])\n",
        "\n",
        "#Train model\n",
        "model.fit(X_train, y_train, epochs=30, verbose=1)\n",
        "# Save the model to a file\n",
        "model.save('TensorFlowModel.keras')\n",
        "\n",
        "test_loss, test_mae = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test loss (MSE): {test_loss:.4f}\")\n",
        "print(f\"Test MAE: {test_mae:.4f}\")"
      ],
      "metadata": {
        "id": "fs6U9xbNuz3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f7e26a9-fcab-428a-8ec7-78e02be1a25c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 6.3616 - mean_absolute_error: 2.2414\n",
            "Epoch 2/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 315ms/step - loss: 6.0903 - mean_absolute_error: 2.1922\n",
            "Epoch 3/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 5.8252 - mean_absolute_error: 2.1431\n",
            "Epoch 4/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 5.5666 - mean_absolute_error: 2.0940\n",
            "Epoch 5/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 5.3145 - mean_absolute_error: 2.0451\n",
            "Epoch 6/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 5.0689 - mean_absolute_error: 1.9963\n",
            "Epoch 7/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 4.8300 - mean_absolute_error: 1.9477\n",
            "Epoch 8/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 4.5977 - mean_absolute_error: 1.8993\n",
            "Epoch 9/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 4.3722 - mean_absolute_error: 1.8510\n",
            "Epoch 10/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 4.1534 - mean_absolute_error: 1.8031\n",
            "Epoch 11/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 3.9413 - mean_absolute_error: 1.7553\n",
            "Epoch 12/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.7361 - mean_absolute_error: 1.7079\n",
            "Epoch 13/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 3.5376 - mean_absolute_error: 1.6608\n",
            "Epoch 14/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 3.3458 - mean_absolute_error: 1.6140\n",
            "Epoch 15/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.1608 - mean_absolute_error: 1.5675\n",
            "Epoch 16/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.9825 - mean_absolute_error: 1.5215\n",
            "Epoch 17/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.8109 - mean_absolute_error: 1.4758\n",
            "Epoch 18/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.6459 - mean_absolute_error: 1.4305\n",
            "Epoch 19/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.4874 - mean_absolute_error: 1.3857\n",
            "Epoch 20/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.3353 - mean_absolute_error: 1.3414\n",
            "Epoch 21/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 2.1897 - mean_absolute_error: 1.2975\n",
            "Epoch 22/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 2.0504 - mean_absolute_error: 1.2542\n",
            "Epoch 23/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1.9172 - mean_absolute_error: 1.2113\n",
            "Epoch 24/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.7902 - mean_absolute_error: 1.1690\n",
            "Epoch 25/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.6691 - mean_absolute_error: 1.1273\n",
            "Epoch 26/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.5539 - mean_absolute_error: 1.0862\n",
            "Epoch 27/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1.4444 - mean_absolute_error: 1.0456\n",
            "Epoch 28/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1.3405 - mean_absolute_error: 1.0057\n",
            "Epoch 29/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1.2420 - mean_absolute_error: 0.9664\n",
            "Epoch 30/30\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.1489 - mean_absolute_error: 0.9277\n",
            "1/1 - 0s - 204ms/step - loss: 1.1840 - mean_absolute_error: 0.9482\n",
            "Test loss (MSE): 1.1840\n",
            "Test MAE: 0.9482\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### TENSORFLOR LITE BASIC WORKFLOW\n",
        "\n",
        "# Load model\n",
        "modelo_a_convertir= load_model('TensorFlowModel.keras')\n",
        "# Convert model to TF Lite\n",
        "lite_model= tf.lite.TFLiteConverter.from_keras_model(modelo_a_convertir)\n",
        "lite_model= lite_model.convert()\n",
        "# Save TF Lite model to a file\n",
        "tf_lite_model_file= pathlib.Path('modelLite.tflite')\n",
        "tf_lite_model_file.write_bytes(lite_model)"
      ],
      "metadata": {
        "id": "OcQ6-6l8sHgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8381367-37e3-4a29-ee97-c3169125861a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpvtymbnw4'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 1), dtype=tf.float32, name='input_layer_2')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139345321285712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321294160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321288400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321294544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321290896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321293392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1776"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up input/output tensors\n",
        "interpreter=tf.lite.Interpreter(model_content=lite_model)\n",
        "interpreter.allocate_tensors()\n",
        "input_details= interpreter.get_input_details()\n",
        "output_details= interpreter.get_output_details()\n",
        "# Set input values\n",
        "to_predict= np.array([[3.3]],dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'],to_predict)\n",
        "# Run inference\n",
        "interpreter.invoke()\n",
        "# Get outputs\n",
        "tflite_results= interpreter.get_tensor(output_details[0]['index'])\n",
        "print(\"La prediccion para 3.3 V es: {:.2f}V y el real es de 0.33V\".format(tflite_results[0][0]))"
      ],
      "metadata": {
        "id": "l5bEfUbL89In",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47607c58-68ae-4798-c28a-d12f624b11e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La prediccion para 3.3 V es: 0.80V y el real es de 0.33V\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vision model with TensorFlow Lite\n",
        "\n",
        "In this part of the assignment, you should import a small pre-trained model for a vision application that takes at most 1 MB. Then you should follow the TensorFlow Lite workflow until you are able to run inference and obtain the same results as with TensorFlow."
      ],
      "metadata": {
        "id": "xpPgM38-6tFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "O_5rOBV65ORf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_fashion(images):\n",
        "    images = images[..., np.newaxis]  # (N, 28, 28, 1)\n",
        "    images = tf.image.resize(images, (96, 96))  # Redimensionar\n",
        "    images = tf.image.grayscale_to_rgb(images)  # Convertir a 3 canales RGB\n",
        "    images = preprocess_input(images)\n",
        "    return images"
      ],
      "metadata": {
        "id": "a_17xJlL6dkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset the fashion Mnist, con solo 200 imagenes"
      ],
      "metadata": {
        "id": "3DRKz7Kb5Ygz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "train_images, train_labels = train_images[:200], train_labels[:200]\n",
        "test_images, test_labels = test_images[:50], test_labels[:50]\n",
        "\n",
        "\n",
        "train_images = preprocess_fashion(train_images)\n",
        "test_images = preprocess_fashion(test_images)\n",
        "base_model = MobileNetV2(input_shape=(96, 96, 3),\n",
        "                         include_top=False,\n",
        "                         weights='imagenet')\n",
        "\n",
        "base_model.trainable = False\n",
        "model = models.Sequential([\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=20, validation_data=(test_images, test_labels))\n",
        "model.save('Pre-treining.keras')\n",
        "\n",
        "loss, accuracy = model.evaluate(test_images, test_labels)\n",
        "print(f\"Pérdida: {loss:.4f}, Precisión: {accuracy:.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3XnC5ku2B9m",
        "outputId": "a45e0393-c618-4e41-f012-13ac1a2e306a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_96_no_top.h5\n",
            "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Epoch 1/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 2s/step - accuracy: 0.1763 - loss: 2.7939 - val_accuracy: 0.4200 - val_loss: 1.7606\n",
            "Epoch 2/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.5902 - loss: 1.3507 - val_accuracy: 0.5400 - val_loss: 1.2943\n",
            "Epoch 3/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.7212 - loss: 0.9054 - val_accuracy: 0.6600 - val_loss: 1.1000\n",
            "Epoch 4/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.8444 - loss: 0.5974 - val_accuracy: 0.6400 - val_loss: 1.0322\n",
            "Epoch 5/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.9167 - loss: 0.4051 - val_accuracy: 0.6600 - val_loss: 0.9625\n",
            "Epoch 6/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9398 - loss: 0.3184 - val_accuracy: 0.6600 - val_loss: 0.8952\n",
            "Epoch 7/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9534 - loss: 0.2695 - val_accuracy: 0.6600 - val_loss: 0.8781\n",
            "Epoch 8/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9496 - loss: 0.2367 - val_accuracy: 0.7000 - val_loss: 0.9000\n",
            "Epoch 9/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9491 - loss: 0.2304 - val_accuracy: 0.7000 - val_loss: 0.9287\n",
            "Epoch 10/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9918 - loss: 0.1806 - val_accuracy: 0.6800 - val_loss: 0.9368\n",
            "Epoch 11/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.9796 - loss: 0.1649 - val_accuracy: 0.6600 - val_loss: 0.9089\n",
            "Epoch 12/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9742 - loss: 0.1565 - val_accuracy: 0.6600 - val_loss: 0.8976\n",
            "Epoch 13/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9838 - loss: 0.1275 - val_accuracy: 0.6800 - val_loss: 0.8980\n",
            "Epoch 14/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9806 - loss: 0.1339 - val_accuracy: 0.6800 - val_loss: 0.9171\n",
            "Epoch 15/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.9815 - loss: 0.1130 - val_accuracy: 0.7000 - val_loss: 0.8899\n",
            "Epoch 16/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9931 - loss: 0.1067 - val_accuracy: 0.6800 - val_loss: 0.8786\n",
            "Epoch 17/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9881 - loss: 0.0939 - val_accuracy: 0.7000 - val_loss: 0.9046\n",
            "Epoch 18/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9931 - loss: 0.0805 - val_accuracy: 0.7000 - val_loss: 0.9066\n",
            "Epoch 19/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.9932 - loss: 0.0704 - val_accuracy: 0.7000 - val_loss: 0.9133\n",
            "Epoch 20/20\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.9784 - loss: 0.0889 - val_accuracy: 0.6800 - val_loss: 0.9138\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - accuracy: 0.7242 - loss: 0.8199\n",
            "Pérdida: 0.9138, Precisión: 0.6800\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "modelo_a_convertir= load_model('Pre-treining.keras')\n",
        "# Convert model to TF Lite\n",
        "Prelite_model= tf.lite.TFLiteConverter.from_keras_model(modelo_a_convertir)\n",
        "Prelite_model= Prelite_model.convert()\n",
        "# Save TF Lite model to a file\n",
        "tf_lite_premodel_file= pathlib.Path('Pre-trainingModelLite.tflite')\n",
        "tf_lite_premodel_file.write_bytes(Prelite_model)"
      ],
      "metadata": {
        "id": "AJ5XXd1o75Sv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba04f3b0-32cb-4efb-d398-8c82d1a9d5d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved artifact at '/tmp/tmpgiig0pqy'. The following endpoints are available:\n",
            "\n",
            "* Endpoint 'serve'\n",
            "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 96, 96, 3), dtype=tf.float32, name='input_layer_4')\n",
            "Output Type:\n",
            "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
            "Captures:\n",
            "  139345321285520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317850832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317842192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321236368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321288016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317851024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317842000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317843152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317843344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317850448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317841616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317840080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317839504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317842384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317841808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317848144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317851984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317848720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317849872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317843536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317847760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317849488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317849104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317849680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317844112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317845456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317853328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317853904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317853136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317852752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317855056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317848336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902930512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317854672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317852560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902932240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902932624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902933008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902932816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902931472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902934160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902934544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902934928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902934736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902930896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902936080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902936464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902936848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902936656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902931856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902938000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902938384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902938768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902938576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902933776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902939920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902940304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902940688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902940496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902935696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902941840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902942224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902942608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902942416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902937616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902943760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902944144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902944528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902944336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902939536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902945680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902945296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902943376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902946256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902941456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902944912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902276304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902275152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902276496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902275344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902277840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902278224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902278608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902278416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902276688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902279760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902280144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902280528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902280336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902275920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902281680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902282064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902282448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902282256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902277456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902283600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902283984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902284368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902284176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902279376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902285520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902285904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902286288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902286096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902281296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902287440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902287824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902288208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902288016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902283216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902289360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902289744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902290128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902289936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902285136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902291280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902287056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901883088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902290896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902288976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901883664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901884048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901884432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901884240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901882896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901885584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901885968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901886352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901886160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901882128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901887504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901887888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901888272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901888080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901882320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901889424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901889808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901890192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901890000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901885200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901891344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901891728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901892112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901891920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901887120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901893264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901893648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901894032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901893840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901889040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901895184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901895568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901895952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901895760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901890960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901897104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901896720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901894800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901897680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901892880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344901896336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902767824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902766864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902767632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902766672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902769360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902769744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902770128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902769936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902767056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902771280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902771664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902772048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902771856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902768208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902773200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902773584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902773968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902773776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902768976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902775120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902775504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902775888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902775696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902770896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902777040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902777424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902777808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902777616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902772816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902778960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902779344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902779728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902779536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902774736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902780880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902781264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902781648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902781456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902776656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902782800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902778576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079337616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902782416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139344902780496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079338768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079339152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079339536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079339344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079338000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079340688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079341072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079341456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079341264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079337040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079342608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079342992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079343376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079343184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079338384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079344528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079344912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079345296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079345104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079340304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079346448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079346832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079347216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079347024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079342224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079348368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079348752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079349136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079348944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079344144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079350288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079350672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079351056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079350864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079346064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079352208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079351824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079349904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079352784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079347984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079351440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079715024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079714448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079715216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079714256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079716560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079716944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079717328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079717136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079715408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079718480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079718864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079719248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079719056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345079713872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345317841040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
            "  139345321282832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8915432"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cargar modelo TFLite\n",
        "interpreter = tf.lite.Interpreter(model_path=\"Pre-trainingModelLite.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Lista de clases para mostrar\n",
        "clases = [\n",
        "    \"Camiseta/top\", \"Pantalón\", \"Suéter\", \"Vestido\", \"Abrigo\",\n",
        "    \"Sandalia\", \"Camisa\", \"Zapatilla deportiva\", \"Bolso\", \"Bota tobillo\"\n",
        "]\n",
        "\n",
        "correct_predictions = 0\n",
        "num_images = 50  # número de imágenes para inferir\n",
        "\n",
        "for i in range(num_images):\n",
        "    img = test_images[i]\n",
        "    img = np.expand_dims(img, axis=0).astype(np.float32)  # Añadir batch\n",
        "\n",
        "    interpreter.set_tensor(input_details[0]['index'], img)\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predicted_class = np.argmax(output)\n",
        "    true_class = test_labels[i]\n",
        "\n",
        "    print(f\"Imagen {i}: Predicción: {clases[predicted_class]}, Etiqueta real: {clases[true_class]}\")\n",
        "\n",
        "    if predicted_class == true_class:\n",
        "        correct_predictions += 1\n",
        "\n",
        "accuracy = correct_predictions / num_images\n",
        "print(f\"\\nPrecisión en {num_images} imágenes: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqWmBJryGCh9",
        "outputId": "3c57f698-e2dd-4c58-c286-9a4f1875a000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagen 0: Predicción: Bota tobillo, Etiqueta real: Bota tobillo\n",
            "Imagen 1: Predicción: Suéter, Etiqueta real: Suéter\n",
            "Imagen 2: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 3: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 4: Predicción: Camisa, Etiqueta real: Camisa\n",
            "Imagen 5: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 6: Predicción: Pantalón, Etiqueta real: Abrigo\n",
            "Imagen 7: Predicción: Camisa, Etiqueta real: Camisa\n",
            "Imagen 8: Predicción: Sandalia, Etiqueta real: Sandalia\n",
            "Imagen 9: Predicción: Zapatilla deportiva, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 10: Predicción: Abrigo, Etiqueta real: Abrigo\n",
            "Imagen 11: Predicción: Sandalia, Etiqueta real: Sandalia\n",
            "Imagen 12: Predicción: Zapatilla deportiva, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 13: Predicción: Vestido, Etiqueta real: Vestido\n",
            "Imagen 14: Predicción: Abrigo, Etiqueta real: Abrigo\n",
            "Imagen 15: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 16: Predicción: Suéter, Etiqueta real: Suéter\n",
            "Imagen 17: Predicción: Camisa, Etiqueta real: Abrigo\n",
            "Imagen 18: Predicción: Bolso, Etiqueta real: Bolso\n",
            "Imagen 19: Predicción: Camiseta/top, Etiqueta real: Camiseta/top\n",
            "Imagen 20: Predicción: Suéter, Etiqueta real: Suéter\n",
            "Imagen 21: Predicción: Zapatilla deportiva, Etiqueta real: Sandalia\n",
            "Imagen 22: Predicción: Zapatilla deportiva, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 23: Predicción: Bota tobillo, Etiqueta real: Bota tobillo\n",
            "Imagen 24: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 25: Predicción: Camisa, Etiqueta real: Abrigo\n",
            "Imagen 26: Predicción: Abrigo, Etiqueta real: Camisa\n",
            "Imagen 27: Predicción: Camiseta/top, Etiqueta real: Camiseta/top\n",
            "Imagen 28: Predicción: Bota tobillo, Etiqueta real: Bota tobillo\n",
            "Imagen 29: Predicción: Camisa, Etiqueta real: Vestido\n",
            "Imagen 30: Predicción: Bolso, Etiqueta real: Bolso\n",
            "Imagen 31: Predicción: Bolso, Etiqueta real: Bolso\n",
            "Imagen 32: Predicción: Sandalia, Etiqueta real: Vestido\n",
            "Imagen 33: Predicción: Camisa, Etiqueta real: Vestido\n",
            "Imagen 34: Predicción: Camiseta/top, Etiqueta real: Bolso\n",
            "Imagen 35: Predicción: Camiseta/top, Etiqueta real: Camiseta/top\n",
            "Imagen 36: Predicción: Zapatilla deportiva, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 37: Predicción: Sandalia, Etiqueta real: Sandalia\n",
            "Imagen 38: Predicción: Zapatilla deportiva, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 39: Predicción: Bota tobillo, Etiqueta real: Bota tobillo\n",
            "Imagen 40: Predicción: Camiseta/top, Etiqueta real: Camisa\n",
            "Imagen 41: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 42: Predicción: Camiseta/top, Etiqueta real: Vestido\n",
            "Imagen 43: Predicción: Bota tobillo, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 44: Predicción: Abrigo, Etiqueta real: Camisa\n",
            "Imagen 45: Predicción: Bota tobillo, Etiqueta real: Zapatilla deportiva\n",
            "Imagen 46: Predicción: Suéter, Etiqueta real: Suéter\n",
            "Imagen 47: Predicción: Pantalón, Etiqueta real: Pantalón\n",
            "Imagen 48: Predicción: Camisa, Etiqueta real: Suéter\n",
            "Imagen 49: Predicción: Camisa, Etiqueta real: Suéter\n",
            "\n",
            "Precisión en 50 imágenes: 68.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-training quantization\n",
        "Finally, in this part of the assignment you should activate quantization and convert the model again. Compare model size and accuracy of the compressed TensorFlow Lite model by using various configurations (investigate how) and against the uncompressed baseline."
      ],
      "metadata": {
        "id": "T25pOIS7kcym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# CARGAR MODELO ENTRENADO\n",
        "model = tf.keras.models.load_model(\"Pre-treining.keras\")\n",
        "input_shape = model.input_shape[1:3]\n",
        "print(\" Tamaño de entrada del modelo:\", input_shape)\n",
        "\n",
        "# CARGAR SOLO 100 IMÁGENES DE TEST\n",
        "(_, _), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_test = x_test[:100]\n",
        "y_test = y_test[:100]\n",
        "\n",
        "# PREPROCESAR AL TAMAÑO CORRECTO\n",
        "def preprocess(images, target_size):\n",
        "    images = images.astype(\"float32\") / 255.0\n",
        "    images = np.expand_dims(images, -1)\n",
        "    images = tf.convert_to_tensor(images)\n",
        "    images = tf.image.grayscale_to_rgb(images)\n",
        "    images = tf.image.resize(images, target_size)\n",
        "    return images.numpy()\n",
        "\n",
        "x_test = preprocess(x_test, input_shape)\n",
        "\n",
        "# REPRESENTATIVE DATASET SOLO CON 100 IMÁGENES, este se usa para calibrar\n",
        "def representative_dataset():\n",
        "    for i in range(100):\n",
        "        yield [x_test[i:i+1].astype(np.float32)]\n",
        "\n",
        "def save_tflite_model(model, filename, quant_type=None, rep_data=None, int8_io=False):\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    if quant_type == \"default\":\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    if rep_data is not None:\n",
        "        converter.representative_dataset = rep_data\n",
        "    if int8_io:\n",
        "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "        converter.inference_input_type = tf.uint8\n",
        "        converter.inference_output_type = tf.uint8\n",
        "    tflite_model = converter.convert()\n",
        "    with open(filename, \"wb\") as f:\n",
        "        f.write(tflite_model)\n",
        "    print(f\" Modelo guardado: {filename}\")\n",
        "\n",
        "# convertir a cada uno de los modelos\n",
        "save_tflite_model(model, \"mobilenet_baseline.tflite\")\n",
        "save_tflite_model(model, \"mobilenet_dynamic.tflite\", quant_type=\"default\")\n",
        "save_tflite_model(model, \"mobilenet_weight_quant.tflite\", quant_type=\"default\")\n",
        "save_tflite_model(model, \"mobilenet_fullint.tflite\", quant_type=\"default\", rep_data=representative_dataset, int8_io=True)\n",
        "\n",
        "# Evaluacion de los modelos\n",
        "def evaluate_tflite_model(tflite_path):\n",
        "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
        "    interpreter.allocate_tensors()\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "    correct = 0\n",
        "    for i in range(100):\n",
        "        input_tensor = x_test[i:i+1].astype(input_details[0]['dtype'])\n",
        "        interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "        interpreter.invoke()\n",
        "        output = interpreter.get_tensor(output_details[0]['index'])\n",
        "        pred = np.argmax(output)\n",
        "        if pred == y_test[i]:\n",
        "            correct += 1\n",
        "    del interpreter\n",
        "    gc.collect()\n",
        "    return correct / 100\n"
      ],
      "metadata": {
        "id": "iLlXfntabf0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Función para obtener tamaño en KB\n",
        "def model_size(path):\n",
        "    return os.path.getsize(path) / 1024"
      ],
      "metadata": {
        "id": "aUQrN7gKCwcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_keras = model_size(\"Pre-treining.keras\")\n"
      ],
      "metadata": {
        "id": "vHWLDOTop62b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# === 8. MOSTRAR RESULTADOS ===\n",
        "\n",
        "# Evaluar precisión\n",
        "precision_keras = model.evaluate(x_test, y_test, verbose=0)[1]\n",
        "precision_dynamic = evaluate_tflite_model(\"mobilenet_dynamic.tflite\")\n",
        "precision_weight = evaluate_tflite_model(\"mobilenet_weight_quant.tflite\")\n",
        "precision_fullint = evaluate_tflite_model(\"mobilenet_fullint.tflite\")\n",
        "\n",
        "# Guardar modelo keras si no lo has guardado ya\n",
        "model.save(\"mobilenet_model.keras\")\n",
        "\n",
        "\n",
        "# Tamaños\n",
        "size_baseline = model_size(\"mobilenet_baseline.tflite\")\n",
        "size_dynamic = model_size(\"mobilenet_dynamic.tflite\")\n",
        "size_weight = model_size(\"mobilenet_weight_quant.tflite\")\n",
        "size_fullint = model_size(\"mobilenet_fullint.tflite\")\n",
        "\n",
        "# Crear DataFrame\n",
        "data = {\n",
        "    \"Modelo\": [\n",
        "        \"Original Keras (.keras)\",\n",
        "        \"Baseline TFLite\",\n",
        "        \"Dynamic Quantization\",\n",
        "        \"Weight Quantization\",\n",
        "        \"Full Integer Quantization\"\n",
        "    ],\n",
        "    \"Precisión\": [\n",
        "        round(precision_keras, 4),\n",
        "        round(0.68,4),\n",
        "        round(precision_dynamic, 4),\n",
        "        round(precision_weight, 4),\n",
        "        round(precision_fullint, 4)\n",
        "    ],\n",
        "    \"Tamaño (KB)\": [\n",
        "        round(size_keras, 2),\n",
        "        round(size_baseline, 2),\n",
        "        round(size_dynamic, 2),\n",
        "        round(size_weight, 2),\n",
        "        round(size_fullint, 2)\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Mostrar tabla con estilo más grande\n",
        "display(HTML(df.to_html(index=False, classes='table table-striped table-bordered table-hover', border=1)))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "e987c7Q4g3m0",
        "outputId": "f51ac559-0869-4686-9df6-370c9f7f16cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe table table-striped table-bordered table-hover\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Modelo</th>\n",
              "      <th>Precisión</th>\n",
              "      <th>Tamaño (KB)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Original Keras (.keras)</td>\n",
              "      <td>0.66</td>\n",
              "      <td>9540.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Baseline TFLite</td>\n",
              "      <td>0.68</td>\n",
              "      <td>8706.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Dynamic Quantization</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2459.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Weight Quantization</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2459.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Full Integer Quantization</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2658.59</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discusión de resultados#"
      ],
      "metadata": {
        "id": "5gYh6L_7YS0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*TensorFlow Lite workflow*"
      ],
      "metadata": {
        "id": "IyLEwBoUuVZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la primera etapa del presente trabajo se elaboró una versión ligera de un regresor lineal el cual presentó un desempeño bastante aceptable, puesto que, aunque no dió un valor exacto para un valor de entrada de 10 entregó 0.8 el cual es un valor no muy exacto, sin embargo, es concordante con la función de pérdida obtenida en el entrenamiento del modelo.\n",
        "\n",
        "De esto, se puede resaltar y se evidencia que el modelo no logró el mejor entrenzmiento."
      ],
      "metadata": {
        "id": "GV_CP6t-ugDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vision model with TensorFlow Lite*"
      ],
      "metadata": {
        "id": "f3GP0_jhxg18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para la segunda parte, se utilizó el modelo MobileNetV2, el cual es una red neuronal convolucional preentrenada con un conjunto de datos que incluye múltiples clases. A este se le añadió solo una capa densa, la cual es de 10 neuronas, debido a que se clasifican 10 clases en el presente trabajo.\n",
        "\n",
        "Se llevó a cabo el proceso de conversión para generar una versión ligera de este modelo (modelo lite), obteniéndose como resultado un comportamiento esperado: no se presentaron pérdidas en la precisión. Esto se debe a que la arquitectura de la red permanece sin modificaciones, ya que los pesos y las funciones de activación se conservan intactos. El único componente modificado fue el intérprete, lo que concuerda con el hecho de que la precisión del modelo lite se mantenga en relación con la obtenida en el modelo original de TensorFlow."
      ],
      "metadata": {
        "id": "ncbRjwlzyOSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Post-training quantization*"
      ],
      "metadata": {
        "id": "pv8NFSwvzYi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, se realizó la cuantización del modelo previamente elaborado (modelo preentrenado con ajuste en la capa de salida) y se compararon sus distintas versiones: cuantizadas, original y Lite. Los resultados se presentan en la tabla anterior, donde se resumen tanto la precisión como el tamaño (en kilobytes) de cada versión del modelo.\n",
        "\n",
        "Se observa que la mayoría de las versiones cuantizadas conservaron una precisión muy similar a la del modelo original y al modelo Lite, con excepción de la versión completamente entera en 8 bits (full integer quantization), la cual obtuvo una precisión de 0.12, siendo el modelo con peor rendimiento durante la inferencia. Este resultado evidencia que la elección del tipo de cuantización tiene un impacto significativo dependiendo del contexto de aplicación. En este caso particular, los pesos perdieron considerable precisión bajo cuantización Int8, lo cual sugiere que la pérdida de componentes decimales provocó una pérdida sustancial de información.\n",
        "\n",
        "Por otro lado, todas las versiones cuantizadas presentaron una reducción considerable en el tamaño de almacenamiento, lo cual representa una ventaja importante para su implementación en sistemas embebidos o dispositivos con recursos limitados, manteniendo una adecuada relación entre eficiencia y precisión."
      ],
      "metadata": {
        "id": "DokzusM0zk83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Conclusiones#"
      ],
      "metadata": {
        "id": "VhMQTRQiYXdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* La conversión a formato TensorFlow Lite conserva un rendimiento comparable al modelo original\n",
        "El modelo en formato Baseline TFLite mantuvo una precisión de 0.68, muy cercana a la del modelo original en Keras (0.66), con una reducción notable en tamaño (de 9540.05 KB a 8706.48 KB). Esto confirma que la conversión a TensorFlow Lite no compromete el desempeño del modelo y es adecuada para entornos con recursos limitados.\n",
        "Cabe aclarar que la ligera diferencia de precisión (+0.02) no representa una mejora real del modelo, ya que ambos comparten la misma arquitectura y pesos. Este cambio puede atribuirse a fluctuaciones menores o aciertos puntuales durante la inferencia, posiblemente debidos a ligeras diferencias en la implementación del intérprete o en la forma en que se procesan ciertas operaciones internamente.\n",
        "\n",
        "* Las técnicas de cuantización permiten una compresión significativa sin afectar la precisión en la mayoría de los casos\n",
        "Tanto la cuantización dinámica como la cuantización de pesos redujeron el tamaño del modelo a 2459.89 KB (más del 70% de reducción respecto al original) y mantuvieron la misma precisión de 0.66. Esto demuestra que es posible optimizar modelos para dispositivos embebidos conservando su exactitud.\n",
        "\n",
        "* La cuantización entera completa (Int8) afecta significativamente la precisión del modelo\n",
        "Aunque esta versión logró una reducción sustancial en el tamaño del archivo (2658.59 KB), el modelo con Full Integer Quantization experimentó una caída drástica en la precisión, alcanzando solo 0.12. Este resultado evidencia que la conversión total a enteros de 8 bits afecta negativamente el rendimiento en tareas de inferencia cuando el modelo depende de una representación más precisa de los pesos y activaciones.\n",
        "En este caso, la eliminación de las componentes decimales provocó una pérdida significativa de información útil para el aprendizaje, comprometiendo la capacidad del modelo para generalizar y realizar predicciones adecuadas. Este comportamiento sugiere que la cuantización entera completa no es adecuada para todos los escenarios, especialmente en modelos sensibles a pequeñas variaciones en los pesos."
      ],
      "metadata": {
        "id": "4ZXfrPe7iZJY"
      }
    }
  ]
}