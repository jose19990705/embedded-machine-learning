{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21619,"status":"ok","timestamp":1749580511696,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"},"user_tz":300},"id":"RP9sunKh921s","outputId":"f358d1d0-867b-4428-940c-0c02ac3faede"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["#Estudiantes:"],"metadata":{"id":"ckcqlCzbv7UJ"}},{"cell_type":"markdown","source":["**Jose Andres Henao Alzate**\n","\n","\n","\n","**Jhon Eduar García Ortiz**"],"metadata":{"id":"oCvye67SvuYG"}},{"cell_type":"markdown","metadata":{"id":"7SFBFiQlYlva"},"source":["# Embedded ML - Lab 2.3: TensorFlow Lite Micro"]},{"cell_type":"code","source":[],"metadata":{"id":"myqL8648OTLU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DTNEs3wxwHHO"},"source":["Tensor Flow Lite Micro (TFLM) is a library that aims to run ML models efficiently on embedded systems. It's a C++ library that provides a version of the TensorFlow Lite interpreter that supports less types of operations and uses less memory. The library also provides helper functions for data pre- and post-processing."]},{"cell_type":"markdown","metadata":{"id":"lQK0RRRuY3rJ"},"source":["### Learning outcomes\n","\n","\n","* Explain the basic concepts associated with TFLM\n","* Use the API to implement the TFLM workflow for an embedded application\n","* Execute TFLM code on a microcontroller-based embedded system"]},{"cell_type":"markdown","metadata":{"id":"l8wat6Kxul5R"},"source":["### TensorFlow Lite Micro workflow"]},{"cell_type":"markdown","metadata":{"id":"P7yMmdHVGlBQ"},"source":["TFLM's high-level workflow is rather simple:\n","* Generate a small TensorFlow model that can fit your target device and contains supported operations.\n","* Convert to a TensorFlow Lite model using the TensorFlow Lite converter, applying quantization if required.\n","* Convert to a C byte array using standard tools and stored it in the read-only program memory on device.\n","* Run inference on device using the TFLM C++ library and process the results."]},{"cell_type":"markdown","metadata":{"id":"B7k1FsSjhAYt"},"source":["### Hello World and Hello Human"]},{"cell_type":"markdown","metadata":{"id":"Hd3ZdaAnhFYd"},"source":["After installing the Arduino IDE and the board files, you should install the Harvard_TinyMLx library that contains the TensorFlow Lite Micro and other resources and examples to build ML apps with Arduino and TFLM. Later on, depending on the application you want to build and the specific hardware to be used, you should install the propper peripheral drivers for communication, sensing and actuating."]},{"cell_type":"markdown","metadata":{"id":"qw2a-MZTh3Za"},"source":["\n","\n","*   Install Arduino IDE 2 from: https://downloads.arduino.cc/arduino-ide/arduino-ide_2.3.2_Linux_64bit.AppImage\n","*   From the boards manager install: Arduino Mbed OS Nano boards\n","*   Allow the linux user to access serial port: `sudo usermod -a -G dialout \\<username\\>` (reboot afterwards)\n","*   From the library manager install: Harvard_TinyMLx\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kpML00lXjmIe"},"source":["Now open the **Hello World** example from the Harvard_TinyMLx library File->Examples->Harvard_TinyMLx in Arduino IDE (also available in [this repo](https://github.com/tinyMLx/arduino-library/tree/main/examples/hello_world)), compile it and run it on the microcontroller board. It is an ML model to predict a sine wave that is used to dim on and off an LED. The Arduino IDE serial monitor should also show interger numbers up and down trying to model a sine wave. This is a test app to make sure that the basic HW and SW elements, including TFLM, are working."]},{"cell_type":"markdown","metadata":{"id":"oYHUC3Jkpb5o"},"source":["Inspect the code to make sure you identify and understand the main parts of the workflow."]},{"cell_type":"markdown","metadata":{"id":"drf1xEruN1E2"},"source":["Running on-device inference using the TFLM C++ library usually involves:\n","\n","* Include the library headers\n","* Include the model header\n","* Load a model\n","* Instantiate operations resolver\n","* Allocate memory\n","* Instantiate interpreter\n","* Read and pre-process input data\n","* Provide inputs to the allocated tensors\n","* Run inference\n","* Get results from the output tensors\n","* Take action based on outputs"]},{"cell_type":"markdown","metadata":{"id":"Hl9oeI3e7Aq7"},"source":["After you have succesfully run the Hello World example, move on to running the **Person Detection** example from the same library. Explore the code in detail to understand how to handle the **camera**."]},{"cell_type":"markdown","metadata":{"id":"VNXSXWHrsJ9x"},"source":["### TinyML application development"]},{"cell_type":"markdown","metadata":{"id":"o7a8wQ45sT3c"},"source":["ML applications that run on embeded systems with very limited resources are often called TinyML. In this lab the goal is to develop a simple TinyML application that uses computer vision up to its deployment on the target embedded device: **Arduino Nano 33 BLE.**\n","\n","Follow these steps in order to develop your TinyML application:\n","\n","1. Select two visual objects that are radically different and  assemble a dataset that contains at least hundreds or thousands of examples. You can create the images yourslef or extract them from a public database and apply data augmentation.\n","\n","2. Design and train a model to classify between the two chosen objects. You can build a dense or CNN model from scratch, or use transfer learning, but you should always keep in mind the very limited memory resources of the target device as well as the image properties of the embedded camera.\n","\n","3.   Export the trained model to a file and convert it to a C header by running the following linux command: `xxd -i converted_model.tflite > converted_model_data.h`\n","\n","4.   Develop an Arduino code based on the Hello World and Person Detection examples, to detect whether any of the two objects are present on the camera view. Indicate the result through the RGB LED.\n","\n","Include in your notebook submission both the code you developed to build the model as well as the C++ codes for the MCU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yRdxSZmU-T8f","executionInfo":{"status":"ok","timestamp":1749580909002,"user_tz":300,"elapsed":107575,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"}},"outputId":"59ace561-8b01-4b0f-b2b0-fc808a4b276c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"," Datos cargados: 472 imágenes de entrenamiento, 92 de prueba\n"]}],"source":["# Conecta Google Drive para acceder a los archivos almacenados allí\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Importación de librerías necesarias para el procesamiento y entrenamiento\n","import os\n","import numpy as np\n","import cv2\n","from sklearn.preprocessing import LabelEncoder\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers, models\n","\n","# Parámetros iniciales\n","IMG_SIZE = (100, 100)  # Tamaño al que se redimensionarán las imágenes\n","base_path = '/content/drive/MyDrive/EML_2_3'  # Ruta base en Google Drive\n","categories = ['banano', 'manzana']  # Clases o etiquetas que se usarán\n","\n","# Función para cargar las imágenes desde carpetas según su etiqueta\n","def cargar_datos(path):\n","    X = []  # Lista para almacenar las imágenes\n","    y = []  # Lista para almacenar las etiquetas correspondientes\n","    for label in categories:\n","        carpeta = os.path.join(path, label)  # Ruta a la carpeta de cada clase\n","        for nombre_img in os.listdir(carpeta):  # Recorre cada archivo en la carpeta\n","            ruta_img = os.path.join(carpeta, nombre_img)\n","            img = cv2.imread(ruta_img, cv2.IMREAD_GRAYSCALE)  # Carga la imagen en escala de grises\n","            if img is not None:\n","                img = cv2.resize(img, IMG_SIZE)  # Redimensiona la imagen\n","                X.append(img)  # Añade la imagen a la lista\n","                y.append(label)  # Añade su etiqueta correspondiente\n","    return np.array(X), np.array(y)\n","\n","# Carga las imágenes de entrenamiento y prueba desde las carpetas correspondientes\n","X_train, y_train = cargar_datos(os.path.join(base_path, 'train'))\n","X_test, y_test = cargar_datos(os.path.join(base_path, 'test'))\n","\n","# Muestra cuántas imágenes se han cargado en total\n","print(f\"Datos cargados: {X_train.shape[0]} imágenes de entrenamiento, {X_test.shape[0]} de prueba\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bInriT7ASP8i"},"outputs":[],"source":["# Normalización y reshaping\n","X_train = X_train / 255.0\n","X_test = X_test / 255.0\n","X_train = X_train.reshape(-1, 100, 100, 1)\n","X_test = X_test.reshape(-1, 100, 100, 1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j9lAmVSt_fPf","executionInfo":{"status":"ok","timestamp":1749583778516,"user_tz":300,"elapsed":31787,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"}},"outputId":"a8c522d0-3547-4487-a2dc-5a79f44347cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.5171 - loss: 0.6908 - val_accuracy: 0.6522 - val_loss: 0.6895\n","Epoch 2/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5182 - loss: 0.6910 - val_accuracy: 0.4674 - val_loss: 0.6874\n","Epoch 3/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5640 - loss: 0.6896 - val_accuracy: 0.5217 - val_loss: 0.6818\n","Epoch 4/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6546 - loss: 0.6834 - val_accuracy: 0.5978 - val_loss: 0.6795\n","Epoch 5/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6502 - loss: 0.6794 - val_accuracy: 0.5978 - val_loss: 0.6772\n","Epoch 6/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.5805 - loss: 0.6758 - val_accuracy: 0.6304 - val_loss: 0.6739\n","Epoch 7/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6528 - loss: 0.6775 - val_accuracy: 0.5761 - val_loss: 0.6701\n","Epoch 8/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6394 - loss: 0.6816 - val_accuracy: 0.5543 - val_loss: 0.6674\n","Epoch 9/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6406 - loss: 0.6754 - val_accuracy: 0.5652 - val_loss: 0.6648\n","Epoch 10/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.5956 - loss: 0.6690 - val_accuracy: 0.6196 - val_loss: 0.6621\n","Epoch 11/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6686 - loss: 0.6712 - val_accuracy: 0.5217 - val_loss: 0.6650\n","Epoch 12/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6329 - loss: 0.6577 - val_accuracy: 0.6196 - val_loss: 0.6566\n","Epoch 13/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7101 - loss: 0.6465 - val_accuracy: 0.5543 - val_loss: 0.6570\n","Epoch 14/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6193 - loss: 0.6770 - val_accuracy: 0.5326 - val_loss: 0.6545\n","Epoch 15/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6691 - loss: 0.6580 - val_accuracy: 0.6196 - val_loss: 0.6526\n","Epoch 16/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6109 - loss: 0.6746 - val_accuracy: 0.6304 - val_loss: 0.6500\n","Epoch 17/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6383 - loss: 0.6670 - val_accuracy: 0.6196 - val_loss: 0.6496\n","Epoch 18/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6623 - loss: 0.6621 - val_accuracy: 0.6413 - val_loss: 0.6475\n","Epoch 19/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6504 - loss: 0.6546 - val_accuracy: 0.6196 - val_loss: 0.6471\n","Epoch 20/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6494 - loss: 0.6525 - val_accuracy: 0.6413 - val_loss: 0.6460\n","Epoch 21/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6736 - loss: 0.6614 - val_accuracy: 0.6304 - val_loss: 0.6456\n","Epoch 22/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6438 - loss: 0.6674 - val_accuracy: 0.5978 - val_loss: 0.6450\n","Epoch 23/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6688 - loss: 0.6498 - val_accuracy: 0.6413 - val_loss: 0.6442\n","Epoch 24/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6703 - loss: 0.6389 - val_accuracy: 0.6413 - val_loss: 0.6465\n","Epoch 25/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6608 - loss: 0.6600 - val_accuracy: 0.6413 - val_loss: 0.6457\n","Epoch 26/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6588 - loss: 0.6583 - val_accuracy: 0.6304 - val_loss: 0.6436\n","Epoch 27/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6771 - loss: 0.6413 - val_accuracy: 0.6522 - val_loss: 0.6434\n","Epoch 28/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6643 - loss: 0.6566 - val_accuracy: 0.6196 - val_loss: 0.6406\n","Epoch 29/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6516 - loss: 0.6691 - val_accuracy: 0.5870 - val_loss: 0.6412\n","Epoch 30/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6956 - loss: 0.6297 - val_accuracy: 0.6196 - val_loss: 0.6397\n","Epoch 31/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6521 - loss: 0.6587 - val_accuracy: 0.6196 - val_loss: 0.6393\n","Epoch 32/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6712 - loss: 0.6502 - val_accuracy: 0.6196 - val_loss: 0.6381\n","Epoch 33/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6784 - loss: 0.6487 - val_accuracy: 0.6413 - val_loss: 0.6369\n","Epoch 34/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6582 - loss: 0.6558 - val_accuracy: 0.6739 - val_loss: 0.6413\n","Epoch 35/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6828 - loss: 0.6489 - val_accuracy: 0.6196 - val_loss: 0.6370\n","Epoch 36/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6855 - loss: 0.6388 - val_accuracy: 0.6522 - val_loss: 0.6366\n","Epoch 37/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7139 - loss: 0.6239 - val_accuracy: 0.6196 - val_loss: 0.6367\n","Epoch 38/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7010 - loss: 0.6293 - val_accuracy: 0.6196 - val_loss: 0.6361\n","Epoch 39/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7012 - loss: 0.6468 - val_accuracy: 0.6413 - val_loss: 0.6352\n","Epoch 40/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6647 - loss: 0.6487 - val_accuracy: 0.5652 - val_loss: 0.6377\n","Epoch 41/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6602 - loss: 0.6521 - val_accuracy: 0.6196 - val_loss: 0.6351\n","Epoch 42/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7104 - loss: 0.6269 - val_accuracy: 0.6304 - val_loss: 0.6343\n","Epoch 43/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6799 - loss: 0.6492 - val_accuracy: 0.5978 - val_loss: 0.6350\n","Epoch 44/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6215 - loss: 0.6696 - val_accuracy: 0.6522 - val_loss: 0.6335\n","Epoch 45/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7062 - loss: 0.6303 - val_accuracy: 0.6413 - val_loss: 0.6342\n","Epoch 46/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6884 - loss: 0.6439 - val_accuracy: 0.6522 - val_loss: 0.6319\n","Epoch 47/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6618 - loss: 0.6328 - val_accuracy: 0.6413 - val_loss: 0.6319\n","Epoch 48/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6941 - loss: 0.6267 - val_accuracy: 0.6522 - val_loss: 0.6326\n","Epoch 49/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6960 - loss: 0.6334 - val_accuracy: 0.6522 - val_loss: 0.6312\n","Epoch 50/50\n","\u001b[1m118/118\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7162 - loss: 0.6256 - val_accuracy: 0.6848 - val_loss: 0.6341\n","\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - accuracy: 0.6940 - loss: 0.6322\n","Precisión en test: 0.68\n"]}],"source":["from tensorflow.keras.optimizers import Adam\n","\n","# Codificación de etiquetas: convierte las clases 'banano' y 'manzana' a valores numéricos (0 y 1)\n","from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","y_train = le.fit_transform(y_train)  # Ajusta y transforma las etiquetas de entrenamiento\n","y_test = le.transform(y_test)        # Transforma las etiquetas de prueba con el mismo codificador\n","\n","# Definición del modelo CNN (Red Neuronal Convolucional)\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Conv2D(3, (3, 3), activation='relu', input_shape=(100, 100, 1)),\n","    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(2, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","# Compilación del modelo: se especifica el optimizador, la función de pérdida y las métricas a seguir\n","model.compile(\n","    optimizer='Adam',                 # Optimizador Adam (eficiente y muy utilizado)\n","    loss='binary_crossentropy',      # Pérdida para clasificación binaria\n","    metrics=['accuracy']             # Métrica a seguir durante entrenamiento y evaluación\n",")\n","\n","# Entrenamiento del modelo con las imágenes de entrenamiento\n","history = model.fit(\n","    X_train, y_train,                # Datos de entrada y sus etiquetas\n","    epochs=50,                       # Número de épocas de entrenamiento\n","    batch_size=4,                    # Tamaño del lote (cuántas imágenes procesa por paso)\n","    validation_data=(X_test, y_test) # Datos de validación para evaluar el rendimiento en cada época\n",")\n","\n","# Evaluación final con el conjunto de prueba\n","loss, acc = model.evaluate(X_test, y_test)\n","print(f'Precisión en test: {acc:.2f}')  # Imprime la precisión obtenida\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":133,"status":"ok","timestamp":1749241746571,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"},"user_tz":300},"id":"M4VOfsUSygQK","outputId":"e91ed244-c93f-45da-bae9-9092b9d97bff"},"outputs":[{"output_type":"stream","name":"stdout","text":["(array([0, 1]), array([228, 244]))\n"]}],"source":["print(np.unique(y_train, return_counts=True))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GB4ra01gmTMG"},"outputs":[],"source":["model.save('ManzanaBananos_best_liviano.keras')"]},{"cell_type":"markdown","metadata":{"id":"MMjAjYZrqiyL"},"source":["#Generación del archivo lite."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":621,"status":"ok","timestamp":1749583779170,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"},"user_tz":300},"id":"aTqibF_YrhU6","outputId":"925a2107-cf1d-4e9d-9da5-0b064b06a0a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmp7_7ow98q'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 100, 1), dtype=tf.float32, name='input_layer_2')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  136236641400016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236641400784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236641399440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236926006672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236641400592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236641401936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]},{"output_type":"execute_result","data":{"text/plain":["3468"]},"metadata":{},"execution_count":23}],"source":["import pathlib\n","\n","# Ruta del modelo previamente entrenado y guardado en formato .keras\n","ruta_modelo = 'ManzanaBananos_best_liviano.keras'\n","\n","# Cargar el modelo desde el archivo .keras\n","modelo = tf.keras.models.load_model(ruta_modelo)\n","\n","# Crear un convertidor para transformar el modelo Keras a formato TensorFlow Lite\n","converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n","\n","# Realizar la conversión a TensorFlow Lite\n","tflite_model = converter.convert()\n","\n","# Definir la ruta de salida y guardar el archivo .tflite generado\n","ruta_salida = pathlib.Path('ManzanasBananos_best_liviano.tflite')\n","ruta_salida.write_bytes(tflite_model)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GryRTZlxBaZk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749583780631,"user_tz":300,"elapsed":1464,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"}},"outputId":"0e9d970b-8f40-4c8b-f13f-0acd6e76902e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saved artifact at '/tmp/tmp0pw_v8aj'. The following endpoints are available:\n","\n","* Endpoint 'serve'\n","  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 100, 100, 1), dtype=tf.float32, name='input_layer_2')\n","Output Type:\n","  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n","Captures:\n","  136236007477840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236007479184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236007479760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236007480336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236641395024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n","  136236641401360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n","  warnings.warn(\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","\n","# Cargar el modelo entrenado previamente desde un archivo .keras\n","modelo = tf.keras.models.load_model(\"/content/ManzanaBananos_best_liviano.keras\")\n","\n","# Crear el convertidor de TensorFlow Lite a partir del modelo cargado\n","converter = tf.lite.TFLiteConverter.from_keras_model(modelo)\n","\n","# Activar la optimización para reducir tamaño y hacer el modelo más eficiente\n","converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","\n","# Definir un conjunto representativo de datos para ayudar en la cuantización\n","# Este paso es clave para que el modelo entienda el rango de los datos reales\n","def representative_dataset():\n","    for i in range(300):\n","        image = X_train[i].astype(np.float32)  # Asegura tipo de dato float32\n","        image = np.expand_dims(image, axis=0)  # Cambia de (100,100,1) a (1,100,100,1)\n","        yield [image]\n","\n","# Asignar el dataset representativo al convertidor\n","converter.representative_dataset = representative_dataset\n","\n","# Configurar la cuantización completa a entero (INT8)\n","converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n","converter.inference_input_type = tf.int8     # La entrada será int8\n","converter.inference_output_type = tf.int8    # La salida será int8\n","\n","# Realizar la conversión a TFLite cuantizado\n","tflite_model_quant = converter.convert()\n","\n","# Guardar el modelo resultante como archivo .tflite\n","with open(\"/content/ManzanaBananos_best_INT8_liviano.tflite\", \"wb\") as f:\n","    f.write(tflite_model_quant)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1749583780662,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"},"user_tz":300},"id":"TKB64Gzjs-bU","outputId":"485f59f5-0ec5-4b47-d0d9-dcf621a4d146"},"outputs":[{"output_type":"stream","name":"stdout","text":["El archivo 'ManzanasBananos_best.tflite' pesa aproximadamente 20.00 KB.\n"]}],"source":["import os\n","\n","ruta_tflite = 'ManzanasBananos_best.tflite'\n","\n","tamaño_bytes = os.path.getsize(ruta_tflite)\n","tamaño_kb = tamaño_bytes / 1024\n","\n","print(f\"El archivo '{ruta_tflite}' pesa aproximadamente {tamaño_kb:.2f} KB.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1749583780666,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"},"user_tz":300},"id":"f8UBfmkMZ-_p","outputId":"928a5f18-ff39-4ecc-f4e3-5c9accaad775"},"outputs":[{"output_type":"stream","name":"stdout","text":["El archivo 'ManzanaBananos_best_INT8.tflite' pesa aproximadamente 10.38 KB.\n"]}],"source":["import os\n","\n","ruta_tflite = 'ManzanaBananos_best_INT8.tflite'\n","\n","tamaño_bytes = os.path.getsize(ruta_tflite)\n","tamaño_kb = tamaño_bytes / 1024\n","\n","print(f\"El archivo '{ruta_tflite}' pesa aproximadamente {tamaño_kb:.2f} KB.\")\n"]},{"cell_type":"markdown","source":["#Generación del array de C++ para el sistema sin cuantización."],"metadata":{"id":"BoNIKK1R848P"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"executionInfo":{"elapsed":2767,"status":"ok","timestamp":1749583783434,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"},"user_tz":300},"id":"bV6mDUHW2kIX","outputId":"719eafdf-9bd9-4f88-9380-526e04df568b"},"outputs":[{"output_type":"stream","name":"stdout","text":[" Instalando 'xxd'...\n"," Convirtiendo a 'model_best.h'...\n","⬇ Descargando 'model.h'...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_bf4a9250-798f-494a-94c5-f1941ac12119\", \"model_best_liviano.h\", 21505)"]},"metadata":{}}],"source":["import os\n","from google.colab import files\n","\n","\n","tflite_filename = \"ManzanasBananos_best_liviano.tflite\"\n","\n","# Verificar existencia\n","if not os.path.exists(tflite_filename):\n","    raise FileNotFoundError(f\" El archivo '{tflite_filename}' no se encuentra en el entorno de trabajo de Colab.\")\n","\n","\n","print(\" Instalando 'xxd'...\")\n","os.system(\"apt-get install xxd -y\")\n","\n","# Convertir a model.h\n","print(\" Convirtiendo a 'model_best.h'...\")\n","os.system(f\"xxd -i {tflite_filename} > model_best_liviano.h\")\n","\n","# Descargar model.h\n","print(\"⬇ Descargando 'model.h'...\")\n","files.download(\"model_best_liviano.h\")\n","\n"]},{"cell_type":"markdown","source":["#Generación del array de C++ para el sistema con cuantización."],"metadata":{"id":"y586fq6K9AlT"}},{"cell_type":"code","source":["import os\n","from google.colab import files\n","\n","\n","tflite_filename = \"ManzanaBananos_best_INT8.tflite\"\n","\n","# Verificar existencia\n","if not os.path.exists(tflite_filename):\n","    raise FileNotFoundError(f\" El archivo '{tflite_filename}' no se encuentra en el entorno de trabajo de Colab.\")\n","\n","\n","print(\" Instalando 'xxd'...\")\n","os.system(\"apt-get install xxd -y\")\n","\n","# Convertir a model.h\n","print(\" Convirtiendo a 'model_INT8.h'...\")\n","os.system(f\"xxd -i {tflite_filename} > model_INT8.h\")\n","\n","# Descargar model.h\n","print(\"⬇ Descargando 'model.h'...\")\n","files.download(\"model_INT8.h\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"u-zjHSue9la2","executionInfo":{"status":"ok","timestamp":1749583785486,"user_tz":300,"elapsed":2050,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"}},"outputId":"be26746d-e999-44e3-caa4-f4e512f5d4b9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Instalando 'xxd'...\n"," Convirtiendo a 'model_INT8.h'...\n","⬇ Descargando 'model.h'...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_f990d020-4273-485f-a003-f2b805f29712\", \"model_INT8.h\", 65676)"]},"metadata":{}}]},{"cell_type":"markdown","source":["* Lo anterior se resume en la siguiente tabla."],"metadata":{"id":"zlukkJpEzFeI"}},{"cell_type":"code","source":["import pandas as pd\n","from IPython.display import display, HTML\n","\n","# Datos de los modelos descritos\n","data = {\n","    \"Característica\": [\n","        \"Arquitectura\",\n","        \"N° Filtros (Conv2D)\",\n","        \"Capas Densas\",\n","        \"Formato Final\",\n","        \"Dataset\",\n","        \"Cantidad de Imágenes\",\n","        \"Precisión Entrenamiento\",\n","        \"Precisión Prueba\",\n","        \"Ventaja Principal\",\n","        \"Desventaja Principal\"\n","    ],\n","    \"Modelo 1 (CNN Simple)\": [\n","        \"Conv2D(3) → MaxPool → Dense(2) → Dense(1)\",\n","        \"3\",\n","        \"2 y 1\",\n","        \"float32 (sin cuantizar)\",\n","        \"Amplio, público (≈1000 imágenes)\",\n","        \"≈ 1000\",\n","        \"98%\",\n","        \"98%\",\n","        \"Alto rendimiento en posiciones conocidas\",\n","        \"Sobreentrenamiento por baja variabilidad\"\n","    ],\n","    \"Modelo 2 (CNN Cuantizado)\": [\n","        \"Conv2D(16) → MaxPool → Dense(20) → Dense(1)\",\n","        \"16\",\n","        \"20 y 1\",\n","        \"INT8 (cuantizado)\",\n","        \"Propio, pequeño (menos imágenes)\",\n","        \"< 1000\",\n","        \"70%\",\n","        \"63%\",\n","        \"Compatible con microcontrolador\",\n","        \"Menor precisión por escasez de datos\"\n","    ]\n","}\n","\n","# Crear DataFrame\n","df = pd.DataFrame(data)\n","\n","# Mostrar tabla HTML con estilo\n","display(HTML(df.to_html(index=False, classes='table table-bordered table-hover', border=1)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"id":"Q8SK6A6AzJFs","executionInfo":{"status":"ok","timestamp":1749608799586,"user_tz":300,"elapsed":32,"user":{"displayName":"JOSE ANDRES HENAO ALZATE","userId":"02478124745201025881"}},"outputId":"5c0edf58-61ea-4401-ee18-4d0f53144a3d"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<table border=\"1\" class=\"dataframe table table-bordered table-hover\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th>Característica</th>\n","      <th>Modelo 1 (CNN Simple)</th>\n","      <th>Modelo 2 (CNN Cuantizado)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>Arquitectura</td>\n","      <td>Conv2D(3) → MaxPool → Dense(2) → Dense(1)</td>\n","      <td>Conv2D(16) → MaxPool → Dense(20) → Dense(1)</td>\n","    </tr>\n","    <tr>\n","      <td>N° Filtros (Conv2D)</td>\n","      <td>3</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <td>Capas Densas</td>\n","      <td>2 y 1</td>\n","      <td>20 y 1</td>\n","    </tr>\n","    <tr>\n","      <td>Formato Final</td>\n","      <td>float32 (sin cuantizar)</td>\n","      <td>INT8 (cuantizado)</td>\n","    </tr>\n","    <tr>\n","      <td>Dataset</td>\n","      <td>Amplio, público (≈1000 imágenes)</td>\n","      <td>Propio, pequeño (menos imágenes)</td>\n","    </tr>\n","    <tr>\n","      <td>Cantidad de Imágenes</td>\n","      <td>≈ 1000</td>\n","      <td>&lt; 1000</td>\n","    </tr>\n","    <tr>\n","      <td>Precisión Entrenamiento</td>\n","      <td>98%</td>\n","      <td>70%</td>\n","    </tr>\n","    <tr>\n","      <td>Precisión Prueba</td>\n","      <td>98%</td>\n","      <td>63%</td>\n","    </tr>\n","    <tr>\n","      <td>Ventaja Principal</td>\n","      <td>Alto rendimiento en posiciones conocidas</td>\n","      <td>Compatible con microcontrolador</td>\n","    </tr>\n","    <tr>\n","      <td>Desventaja Principal</td>\n","      <td>Sobreentrenamiento por baja variabilidad</td>\n","      <td>Menor precisión por escasez de datos</td>\n","    </tr>\n","  </tbody>\n","</table>"]},"metadata":{}}]},{"cell_type":"markdown","source":["### Evaluación y Comparación de Modelos\n","En la búsqueda del modelo óptimo para la aplicación, se evaluaron dos arquitecturas de redes neuronales convolucionales (CNN) con enfoques distintos en cuanto a complejidad, optimización y cantidad de datos utilizados.\n","\n","El primer modelo, de arquitectura sencilla (una capa Conv2D con 3 filtros, seguida de MaxPooling y dos capas densas), fue entrenado con un dataset amplio de aproximadamente 1000 imágenes. Alcanzó una precisión del 98% tanto en entrenamiento como en prueba, pero evidenció sobreentrenamiento, debido a la alta similitud entre las imágenes del conjunto de entrenamiento y validación, lo que limitó su capacidad de generalización a nuevas posiciones o variaciones.\n","\n","El segundo modelo, más profundo (una Conv2D con 16 filtros y una capa densa intermedia de 20 neuronas), fue cuantizado a formato INT8 para ser ejecutable en un microcontrolador con restricciones de memoria y procesamiento. Este fue entrenado con un dataset propio, más pequeño pero ligeramente más diverso, obteniendo una precisión del 70% en entrenamiento y 63% en prueba. Su rendimiento inferior se atribuye principalmente a la escasez y limitada variabilidad del dataset.\n","\n","Cabe destacar que, en la fase de inferencia, el modelo entrenado con mayor cantidad de imágenes (aunque más simple) mostró un desempeño más sólido y consistente. Esto evidencia que un mayor volumen de datos puede compensar, en cierta medida, la simplicidad arquitectónica, mientras que una red más compleja no garantiza mejores resultados si se entrena con un conjunto de datos limitado o poco representativo.\n"],"metadata":{"id":"xalbXYvv2jLx"}},{"cell_type":"markdown","source":["# Conclusiones"],"metadata":{"id":"EDGTsrUP0pDO"}},{"cell_type":"markdown","source":["* En conclusión, los resultados del laboratorio demostraron que modelos convolucionales simples pueden superar a modelos más profundos, sin embargo, esto depende fuertemente de datos de entrenamiento de alta calidad, representativos, diversos y con bajo nivel de ruido. Aunque el primer modelo alcanzó una mayor precisión, su capacidad de generalización se vio afectada por el sobreentrenamiento derivado de la similitud entre imágenes. Por su parte, el segundo modelo, optimizado mediante cuantización para su uso en hardware embebido, mostró un rendimiento inferior debido principalmente a la baja calidad y cantidad del dataset. Estos hallazgos evidencian que el desempeño de un modelo no depende exclusivamente de su arquitectura, sino que está fuertemente condicionado por las características de los datos utilizados para entrenarlo.\n","\n","* Por otra parte, se pudo evidenciar la importancia del proceso de cuantización para lograr la reducción de los recursos de cómputo necesarios para la ejecución de modelos moderadamente profundos. Esto porque se pudo ejecutar, en un kit Arduino 33 BLE, un modelo que inicialmente usaba representacion de float32, pero se optimizó su ocupación en memoria por medio de la cuantización a enteros sin signo de 8 bits.\n","\n","* Esta práctica evidencia el constante intercambio que se debe llevar a cabo para lograr un modelo tanto generalizable como preciso, esto se refiere al equilibrio entre la complejidad en la arquitectura, la representación de sus datos en memoria y, en algunos casos, la latencia en la inferencia según la aplicación.\n","\n","\n","\n"],"metadata":{"id":"t0R1rFxV0rI5"}}],"metadata":{"colab":{"gpuType":"T4","provenance":[{"file_id":"1fqxQZnuNJzv_90KpD6XUZXgQBqLt0V6l","timestamp":1748991745471},{"file_id":"10tbR34kWI-1TwyObb7RxvpSdilpWBq7W","timestamp":1748270007957}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}